[transmogrifier]

pipeline =
    crawler
    typeguess
    template1
    template2
    indexguess
    sitemapper
    attachmentguess
    hideguess
    addfolders
    titleguess
    urltidy
    changetype
    ploneupload
    ploneupdate
    ploneportlets
    plonehide
    publish
    plonepublish
    plonealias



[ploneupload]
blueprint = transmogrify.ploneremote.remoteconstructor
@doc = Adds content to plone via xmlrpc
# see http://pypi.python.org/pypi/transmogrify.ploneremote#remoteconstructor
#
@target = URL: The base url for where all content should be created. Can support basic authentication
@debug = show extra debug information
type-key = _type
path-key = _path
creation-key = _creation_flag
target=${settings:target}
move-condition=python:False
remove-condition=python:False

[ploneupdate]
blueprint = transmogrify.ploneremote.remoteschemaupdater
@doc = Updates content of existing object on a remote plone site via xmlrpc
# see http://pypi.python.org/pypi/transmogrify.ploneremote#remoteschemaupdater
@target = URL: the base url for where all content should be updated. Can support basic authentication
@skip-unmodified = BOOLEAN: if true the modification date will be compared with that on server and updating skipped
@skip-fields = LIST: don't update these fields during update
@skip-existing = BOOLEAN: if creation-key is set then update, otherwise skip
@debug = show extra debug information
#
target = ${ploneupload:target}
condition = python:True
skip-existing = False
skip-unmodified = True
skip-fields =
creation-key = ${ploneupload:creation-key}
headers-key = _content_info
type-key = _type
path-key = _path



[ploneportlets]
blueprint = transmogrify.ploneremote.remoteportlets
@doc = Sets left and right portlets
# see http://pypi.python.org/pypi/transmogrify.ploneremote#remoteschemaupdater
@target = URL: the base url for where all content should be updated. Can support basic authentication
@debug = show extra debug information
#
target = ${ploneupload:target}
type-key = _type
path-key = _path
condition = python:True



[plonehide]
blueprint = transmogrify.ploneremote.remotenavigationexcluder
@doc = Hide items from the navigation
 (hints to which items should be hidden are set earlier in pipeline)
 by default it will hide items not linked to outside of any body text
# see http://pypi.python.org/pypi/transmogrify.ploneremote#remotenavigationexcluder
#
@debug = show extra debug information
target = ${ploneupload:target}
type-key = _type
path-key = _path
exclude-from-navigation-key = _exclude-from-navigation

[publish]
blueprint = collective.transmogrifier.sections.inserter
@doc = Set the workflow transition
#key hint which will be used publish the item.
# This value is later read by [plonepublish] and we can have conditions
# to have different WF transitions for different content types 
# Note that images do not have workflow and they inherit
# permission settings of the parent container
#
@value = TAL: TAL expression to return the transition to workflow
key = string:_transitions
value = python:["publish"]
condition = python:item.get('_type') != 'Image' and not options.get('disabled')

[plonepublish]
blueprint = transmogrify.ploneremote.remoteworkflowupdater
@doc= Publish or otherwise change the workflow state of remote plone content
# see http://pypi.python.org/pypi/transmogrify.ploneremote#remoteworkflowupdater
#
@debug = show extra debug information
transitions = submit publish
transitions-key = _transitions
target = ${ploneupload:target}
type-key = _type
path-key = _path

[plonealias]
blueprint = transmogrify.ploneremote.remoteredirector
@doc = Creates aliases for items that have moved
# see http://pypi.python.org/pypi/transmogrify.ploneremote#remoteredirector
#
target = ${ploneupload:target}
type-key = _type
path-key = _path

[cache]
blueprint = transmogrify.webcrawler.cache
#
@doc= Saves content to disk
# see http://pypi.python.org/pypi/transmogrify.webcrawler#cache
#
@output = DIR: relative directory to store cached downloads
@debug = show extra debug information
output = var/funnelwebcache/${crawler:url}/


[typeguess]
blueprint = transmogrify.webcrawler.typerecognitor
#
@doc = Sets Plone content type based on mime-type
# see http://pypi.python.org/pypi/transmogrify.webcrawler#TypeRecognitor
@condition = TAL: Tal expression returning boolean called for each 'item' 
@debug = show extra debug information

[template1]
blueprint = transmogrify.htmlcontentextractor
title       = text //div[@class='content-view-full']//h1[1]
text        = html //div[@class='content-view-full']

[template2]
blueprint = transmogrify.htmlcontentextractor
@doc = Used if no previous templates matched. see template1 for options
title       = text //div[@id='maincontent-design']//h1[1] | //title
text        = html //div[@id='maincontent-design']

[template3]
blueprint = transmogrify.htmlcontentextractor
@doc = Used if no previous templates matched. see template1 for options

[template4]
blueprint = transmogrify.htmlcontentextractor
@doc = Used if no previous templates matched. see template1 for options

[templateauto]
blueprint = transmogrify.htmlcontentextractor.auto
@doc = Guesses XPaths of content by performing a cluster analysis of all the content not already matched 
# see http://pypi.python.org/pypi/transmogrify.htmlcontentextractor@auto
@condition = TAL: A TAL expression returning boolean called for each 'item'. Turned off by default.
@debug = show extra debug information
condition=python:False

[indexguess]
blueprint = transmogrify.siteanalyser.defaultpage
@doc = Determines an item is a default page for a container if it has many links
 to items in that container even if not contained in that folder 
# see http://pypi.python.org/pypi/transmogrify.siteanalyser#defaultpage
@condition = TAL: tal expression returning boolean called for each 'item' 
@default_pages = LIST: names that indication page should be a defaultpage
@debug = show extra debug information
@min_links = INT: If a page has this many links to a single folder's content it will be moved
@max_uplinks = INT: If a page has more than this many links parent folders then don't more it
condition=python:False
min_links = 2
max_uplinks = 2
default_pages =
    index.html
    index
    index-html
    index.asp
    index.php
default_containers = Folder

[sitemapper]
blueprint = transmogrify.siteanalyser.sitemapper
@doc = Uses a indented html with links in to rearrange those links in the site
@condition = TAL: Which item to use as the sitemap
@debug = show extra debug information
exclude-from-navigation-key = ${hideguess:key}
title-key = title

[attachmentguess]
blueprint = transmogrify.siteanalyser.attach
@doc = Finds items only referenced by one page and moves them into a new folder with the page as the default view
# see http://pypi.python.org/pypi/transmogrify.siteanalyser#makeattachments
#
@condition = TAL: TAL expression returning boolean called for each 'item' 
@debug = show extra debug information
@defaultpage = NAME: name to give created defaultpages
#condition = python: subitem.get('_type') in ['Image']
condition=python:False
defaultpage = index-html

[hideguess]
blueprint = transmogrify.siteanalyser.hidefromnav
@doc = Picks content which won't be shown in the site navigation
@condition = TAL: TAL expression to pick which items should be hidden
key = _exclude-from-navigation
condition = python:0 


[addfolders]
blueprint = transmogrify.siteanalyser.pathsorter
@default_containers = TYPE: Type to set when creating folders
@debug = show extra debug information
default_containers = Folder


[titleguess]
blueprint = transmogrify.siteanalyser.title
@doc = Tries to find better page titles by analysing backlink text
#  see http://pypi.python.org/pypi/transmogrify.siteanalyser#title
#
@condition = TAL: TAL expression returning boolean called for each 'item'
@debug = show extra debug information
@ignore = LIST: don't use backlink text containing these substrings
ignore =
	click
	read more
	more info
	http:
	file:
	img
ignore_re =
	\.\.\. *$
	\. \. \. *$
	^Close$
	^close$
condition=python:False

[urltidy]
blueprint = transmogrify.siteanalyser.urltidy
@doc = Applies title normalisation rules remove invalid chars from urls. It will also ensure all internal links are corrected
@debug = show extra debug information
@link_expr = TAL: TAL expression to set new value of the path
@use_title = TAL: TAL expression to switch id to use the title
@invalid_ids = Rename the reserved words by Plone
# see http://pypi.python.org/pypi/transmogrify.siteanalyser#relinker
#
link_expr = python:item['_path'].replace('index.php/', '').replace('index.php', '')
use_title = python:False
invalid_ids =
    view
    layout
    edit

[changetype]
blueprint = collective.transmogrifier.sections.inserter
@doc = Switch the type of the created object if desired
#
@value = TAL: TAL expression to give the new value for the Type of object.
key = string:_type
condition = python:item.get('_type')
value = python:item['_type']

[saveall]
blueprint = transmogrify.webcrawler.serializer
@action = Should be save or restore
@directory = Destination/Source for above action
@debug = Debug mode

action = save
directory = var/pickles
debug = True

[crawler]
blueprint = transmogrify.webcrawler
@doc = Crawls site or cache for content
@url = URL: the top url to crawl
@start-urls = LIST: additional urls to crawl at the start
@ignore = LIST: list of regex for urls to not crawl
@cache = DIR: local directory to read crawled items from instead of accessing the site directly
@patterns = LIST: Regular expressions to substitute before html is parsed. New line seperated 
@subs = LIST: Text to replace each item in patterns. Must be the same number of lines as patterns 
@maxsize = BYTES: don't crawl anything larger than this
@max = INT: Limit crawling to this number of pages
@ignore_robots = BOOL: Ignore robots.txt for when you really want their content
@debug = show extra debug information
@nskip = INT: skip this many urls before starting the processing.
#
# see http://pypi.python.org/pypi/transmogrify.webcrawler
#

url = ${settings:url}

ignore =
  cgi-bin
  javascript:
cache = ${cache:output}
ignore_robots = false

start-urls = 
